---
layout: post
title: "caffe 学习笔记 1"
subtitle: "caffe 中的 SGD solver"
author: "Johnny"
date: 2016-06-15 19:04:09
header-img: "img/bg-4.jpg"
tags: 
    - deep learning
    - caffe
---


solver是caffe中一个很重要的概念，简单来说solver通过不断 forward和backward计算梯度，用计算得到的梯度对模型进行优化，更新深度神经网络各层的参数，使得loss最小以使模型更好地拟合数据。

# solver的作用及分类 #


在caffe中封装的有以下几种solver：

 - Stochastic Gradient Descent (type: "SGD")
 - AdaDelta (type: "AdaDelta")
 - Adaptive Gradient (type: "AdaGrad")
 - Adam (type: "Adam")
 - Nesterov's Accelerated Gradient (type: "Nesterov")
 - RMSprop (type: "RMSProp")

在以上solver中比较常用的Stochastic Gradient Descent（SGD），也即随机梯度下降，其他的solver我还暂时没有用到过。

solver主要进行以下工作：

 1. 定义并生成一个网络用来训练和测试
 2. 反复迭代对网络进行forward和backward计算并更新网络参数
 3. 根据给定参数定期对训练中的网络进行测试评估
 4. 根据所给参数在训练过程中定期保存网络权重

solver从最开始初始化的网络模型开始在每次迭代计算过程中都将进行：

 1. 调用forward function 计算网络输出和loss
 2. 调用backward function 计算梯度
 3. 根据所选solver类型的不同相应地根据计算得到的梯度更新网络参数
 4. 根据所选学习率、solver的类型和历史数据更新solver状态
 
 

# 理论介绍 #

solver把最优化深度神经网络当做最小化loss的过程。比如有一个含有\\(D\\)个训练数据的数据集，我们最优化（也即最小化)的目标就是这\\(D\\)个数据的平均loss，也即：
$$ L(W)=\frac{1}{\left | D \right |}\sum_{i}^{\left | D \right |}f_{W}(X^{(i)})+\lambda r(W) $$
其中，\\[f_{W}(X^{(i)})\\]是在单个数据\\[X^{(i)}\\]上的loss ,\\[r(W)\\]是以\\[\lambda\\]为权重的一个正则项。在实际应用中\\[{\left | D \right |}\\]一般是一个非常大的数字，所以我们常常用一个远小于\\[{\left | D \right |}\\]的数字$N$代替，每次迭代计算在数据集中随机选\\[N\\]个数据近似代替。
$$ L(W)\approx \frac{1}{N}\sum_{i}^{N}f_{W}(X^{(i)})+\lambda r(W) $$
 solver在forward传播中计算\\[f_{W}\\]，在backward传播中计算梯度\\[\nabla f_{W}\\]。
 
 网络参数按照不同的solver定义的不同规则，结合计算得到的error gradient \\[\nabla f_{W}\\]、正则项的梯度\\[\nabla r(W)\\]更新网络权重。

# SGD solver #


随机梯度下降算法更新网络模型权重$W$时结合梯度\\[\nabla L(W)\\]和前一次迭代的更新权重\\[V_t\\]。学习率\\[\alpha \\]（learning rate）是更新权重时的系数，momentum \\[\mu \\]是上一次迭代权重在这次迭代中权重更新的系数。整个过程可以用以下两个方程表示：
$$ V_{t+1} = \mu V_t + \alpha \nabla L(W_t) $$
$$ W_{t+1} = W_t - V_{t+1} $$
其中\\[W_{t+1}\\]表示在\\[t+1\\]次迭代后的网络权重，\\[V_{t+1}\\] 则是第\\[t+1\\]次迭代中网络权重的更新量。\\[W_{t}\\]和\\[V_{t}\\] 相应是第\\[t\\]次迭代更新后的网络权重和网络权重更新量。

为了在训练中使模型更好、更快地收敛，可能需要对定义solver中的参数（\\[\alpha \\]和\\[\mu \\])进行微调。

# 设置solver参数的一些技巧 #


把学习率\\[\alpha \\]设置为0.01并且在训练过程中间隔一定的迭代次数减小学习率，是一个比较好而且得到经验证明的策略。一般吧momentum \\[\mu\\]设为0.9，通过平滑迭代过程中更新权重的系数，SGD solver在最优化过程中往往更加稳定和迅速。这个策略在ILSVRC2012总被AlexNet采用，caffe也可以通过定义solver各种参数的prototxt文件复现这个策略。示例如下：

```
base_lr: 0.01     # begin training at a learning rate of 0.01 = 1e-2

lr_policy: "step" # learning rate policy: drop the learning rate in "steps"
                  # by a factor of gamma every stepsize iterations

gamma: 0.1        # drop the learning rate by a factor of 10
                  # (i.e., multiply it by a factor of gamma = 0.1)

stepsize: 100000  # drop the learning rate every 100K iterations

max_iter: 350000  # train for 350K iterations total

momentum: 0.9
```

在上面这个prototxt定义的SGD solver中momentum \\[\mu=0.9\\]，在训练的前100000次迭代采用初始学习率base_lr \\[\alpha=0.01\\]，100000次之后的迭代学习率lr \\[\alpha' = \alpha \gamma = (0.01) (0.1) = 0.001 = 10^{-3}\\]，200000次迭代后的学习率lr \\[\alpha'' = 10^{-4}\\]，300000次迭代后直到训练结束（3500000次迭代）学习率lr \\[\alpha''' = 10^{-5}\\]。

缺少部分，待续……

# 参考资料 #
1. [caffe API 文档](http://caffe.berkeleyvision.org/doxygen/classcaffe_1_1Solver.html)
1. [Caffe Tutorial——Solver](http://caffe.berkeleyvision.org/tutorial/solver.html)
1. [Bottou L. Stochastic gradient descent tricks[M]//Neural Networks: Tricks of the Trade. Springer Berlin Heidelberg, 2012: 421-436.](http://link.springer.com/chapter/10.1007/978-3-642-35289-8_25)

