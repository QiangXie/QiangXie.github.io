---
layout: post
title: "L0、L1和L2范数"
subtitle: "常用规则化项理解"
author: "Johnny"
date: 2016-08-22 16:00:09
header-img: "img/caffe_install.jpg"
tags:
    - Machine Learning
---

&#160; &#160; &#160; &#160;机器学习中，我们一直期望学习到一个泛化能力（Generalization）强的函数，只有泛化能力强的模型才能很好地适用于整个样本空间，才能在新的样本点上表现良好。但是训练集通常只是整个样本空间很小的一部分，在训练机器学习模型时，稍有不注意，就可能将训练集中样本的特性当作了全体样本的共性，以偏概全，而造成过拟合（Overfitting）。为了解决这个问题，规则化方法（Regularize)应运而生。所谓规则化方法，就是在训练模型的时候加入一些约束，使得训练得出的模型避免过拟合。一般采用的方法是在模型的损失函数加一个规则化项，最优化的目标函数如下：

$$
w^{*}=argmin_{w}\sum_{i}L(y^{(i)},f(x^{(i)};w))+\lambda\Omega(w)
$$

&#160; &#160; &#160; &#160;其中，第一项对应于模型在训练集上的误差，第二项对应于规则化项。为了使得该目标函数最小，我们既需要训练误差最小，也需要规则化项最小，因此需要在二者之间做到权衡。规则化是结构风险最小化策略的实现，是在经验风险最小化上加一个规则化项（regularizer）或罚项（penalty term）。规则化项一般是模型复杂度的单调递增函数，模型越复杂，规则化值就越大。比如，规则化项可以是模型参数向量的范数。在《统计学习方法》里有这样一段话：*规则化符合奥卡姆剃刀（Occam’s razor）原理。奥卡姆剃刀原理应用于模型选择时变为以下想法：在所有可能选择的模型中，能够很好地解释已知数据并且十分简单才是最好的模型，也就是应该选择的模型。从贝叶斯估计的角度来看，规则化项对应于模型的先验概率。可以假设复杂的模型有较大的先验概率，简单的模型有较小的先验概率。* 所以，引入规则化项，简单来说就是为了降低模型的复杂度，越简单的模型越不容易过拟合。在实际问题中，一般使用L1范数和L2范数，这里也介绍一下L0范数。

#### L0范数和L1范数 ####

&#160; &#160; &#160; &#160; **所谓L0范数就是指向量中的非0的元素个数。** 所以，如果L0范数作为最优化的一部分的时候，则随着这一部分越来越小，也就是非0元素越来越少，也即稀疏化。参数越稀疏，模型越简单。但是L0范数很难优化求解（NP难问题），这使得在实际应用中没法把L0范数作为一个规则化项加入到损失函数里采用最优化算法求解。不过，还有一个L1范数可以实现参数稀疏化。**所谓L1范数，就是向量的元素的绝对值之和。** 即：

$$
\Vert x\Vert_{1}=\sum_{i}\vert x_{i}\vert
$$

&#160; &#160; &#160; &#160;相应地目标函数变为：

$$
w^{*}=argmin_{w}\sum_{i}L(y^{(i)},f(x^{(i)};w))+\lambda\Vert w\Vert_{1}
$$

&#160; &#160; &#160; &#160;这个问题称为最小绝对收缩和选择算子(least absolute shrinkage and selection operator, LASSO)。L1范数也能产生稀疏性，使得 \\(w\\) 中的许多项变成零。为什么L1范数能使参数稀疏化呢？一个直观的解释如下(引自参考资料一)：上面优化的目标函数可以等价成如下形式：

$$
argmin_{w}\sum_{i}L(y^{(i)}, f(x^{(i);w})),s.t.\Vert w \Vert_{1}\leq C
$$

&#160; &#160; &#160; &#160;这里\\(C\\)是一个常数，上面的等价式子就是说，后面的规则化项把参数限制在一个范围内（norm ball)，假设\\(w\\)是二维的，如下图所示，在二维平面上画出目标函数的等高线，越靠近外侧的线代表目标函数的值越大，则norm ball与等高线靠近极小值点方向第一个相交的地方就是规则化条件下的最优解。

![java-javascript](/img/in-post/l0l1l2/l1-ball.svg)

&#160; &#160; &#160; &#160;L1的norm ball和等高线相交的地方肯定是角点，这个角点位置会产生稀疏性。如果 \\(w\\) 是三维，或者更高的维度，会在规则化范围的轮廓处第一次相交，产生稀疏性。因此，L1范数规则化会对特征进行选择，让大部分特征置零（参数为0），从而实现稀疏特征选择，而且降低了模型的复杂度，避免过拟合。因为L1范数在 \\(x=1\\) 处不可导，所以求解的时候需要用次梯度求解。

#### L2范数 ####

&#160; &#160; &#160; &#160; **所谓L2范数就是向量的模长** ，定义如下：

$$
\Vert x\Vert_{2}=\sqrt{\sum_{i}x_{i}^{2}}
$$

&#160; &#160; &#160; &#160;当使用L2范数作为规则化项对目标函数优化过程中对参数的约束时，优化问题可以写成如下形式：

$$
argmin_{w}\sum_{i}L(y^{(i)}, f(x^{(i);w})),s.t.\Vert w \Vert_{2}\leq C
$$

&#160; &#160; &#160; &#160;将损失函数的等高线图和L2-范数规则化约束画在同一个坐标轴下,如下图：

![java-javascript](/img/in-post/l0l1l2/l2-ball.svg)

&#160; &#160; &#160; &#160;L2范数约束对应于坐标轴上一个圆形的norm ball。L2范数会使得参数整体都很小，但是却不会为零，所以参数不是稀疏的。虽然每个参数都很小，但是还在模型的输出起着作用，而且因为参数值比较小，所以模型的输出会很平滑，输出不会因为某一个特征变化输出结果就变化很大，这样模型就会比较鲁棒，上图也能说明这一点。

&#160; &#160; &#160; &#160;以上是我对L0，L1和L2范数的理解，暂时只有这些，以后如果有更加深入的理解再增加内容。

 1.[Sparsity and Some Basics of L1 Regularization][1]

 2.[机器学习中的范数规则化之（一）L0、L1与L2范数][2]

   [1]: http://freemind.pluskid.org/machine-learning/sparsity-and-some-basics-of-l1-regularization/
   [2]: https://blog.csdn.net/zouxy09/article/details/24971995
