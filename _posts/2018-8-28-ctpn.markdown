---
layout: post
title: "论文阅读Detecting Text in Natural Image with Connectionist Text Proposal Network"
subtitle: "一个引入序列信息的文本检测方法"
author: "Johnny"
date: 2018-08-28 18:03:54
header-img: "img/caffe_install.jpg"
tags: 
    - OCR
    - Paper Reading
    
---

## 1. 算法核心内容  ##

&#160; &#160; &#160; &#160;本论文提出了一个使用CNN和LSTM的文本检测算法，CNN学习深度特征，使用LSTM学习使用文本特征序列中的有用信息用于文本检测，获得不错的检测效果。算法整体流程如下文所示：

![java-javascript](/img/in-post/paper-ctpn/architecture.png)


1.算法对输入图片首先使用VGG网络的前五个conv模块提取深度特征，得到大小为`W*H*C`的特征图；

2.对获得的特征图使用SSD中类似的`3*3`卷积核进行滑动卷积得到宽度为`W`的特征序列，和SSD不同的是，每个`3*3`的卷积核卷积的信息并不是用来回归得到检测目标的位置信息，而是用来构造适合LSTM输入的特征序列；

3.将每一行滑动卷积得到的特征序列送入LSTM中，得到`W*256`维的输出；

4.将LSTM输出的`W*256`维的数据通过一个全连接降维到512维；

5.第五步也是最重要的一步，和传统目标检测直接使用区域feature回归出一个独立目标不同，文本检测如果是检测一个单词或者一个字母或者一个字符都是不太可靠的，最好的检测办法是在图片中检测整个文本行。如下图所示：

![java-javascript](/img/in-post/paper-ctpn/difference.png)

&#160; &#160; &#160; &#160;文本检测的目标和通用目标检测算法主要的区别是文本行是一行，是一个细条状的目标。很自然的，对于这样一个细条状的文本行进行检测时，可以看做是检测若干个文本行小部件，这些小部件检测到的内容可以是一个字符或者笔画或者字符的一部分，如上图中的右侧图，一个小部件是一个宽度为16个像素点宽度的小矩形，每个小矩形上下边的坐标靠LSTM输出的feature信息进行预测。预测时使用和Faster-RCNN以及SSD类似的anchor机制，只不过这`k`个anchor是一个竖直anchor：只预测固定宽度的小矩形部件的上下边的y坐标的位置信息。比如`k=10`，意思是这10个anchor的区别是高度大小不同（论文里的anchor小矩形的高度从11个像素点到273个像素点之间）。由于不同小矩形部件的上下边的坐标是可以变化的，所以CTPN可以对前后字符大小不一致的文本行进行检测。回归公式如下：

$$ \begin{gather*}
v_{c} =\left( c_{y} -c^{y}_{a}\right) /h^{a} ,\ y_{h} =log\left( h/h^{a}\right)\\
v^{*}_{c} =\left( c^{*}_{y} -c^{a}_{y}\right) /h^{a} ,\ v^{*}_{h} =log\left( h^{*} /h^{a}\right)\\
\end{gather*} $$

&#160; &#160; &#160; &#160;上面公式中，\\(v^{*}\\)和\\(v\\)分别代表ground truth和预测值和anchor之间的相对误差，\\(c^{a}_{y}\\)和\\(h^{a}\\)分别代表anchor的中心坐标和高度。其实，上面公式跟Faster-RCNN发展起来的目标检测中要计算的相对误差是相似的，只是没有再计算相对anchor宽度的相对误差，而是变成了一个固定值，比如16pixel。

&#160; &#160; &#160; &#160;对于文本上下边也就是竖直方向的坐标信息可以用上述方法得到较为准确的信息，但是对于水平方向的坐标信息的预测则稍显不足。为什么呢？因为如前所述，在对文本行进行小部件检测时，只回归出了文本行中的竖直方向的位置信息，比如中心点y坐标和高度，并没有得到水平方向的小部件的位置信息，水平方向的小部件的宽度是恒定的16个pixel。这样固定的宽度对于文本行非边缘的一个小矩形无所谓，但是对于处于两边的小矩形的宽度预测是不够准确的，比如有些处于边缘的小矩形可能由于和ground truth的交集较少，或者得到的分数不够而被判定为不是文本行内容。如果仅仅以最外侧判定为文本行的宽度作为最后连接起来的文本行的水平坐标信息，则最终获得的检测框会漏掉一些文本信息，如下图所示：

![java-javascript](/img/in-post/paper-ctpn/side.png)

&#160; &#160; &#160; &#160;针对这个问题，本文使用一种叫做Side-refinement的方法改善了CTPN在文本行边界定位不准的问题。简单来说，对于每一个anchor，不仅预测出y坐标信息，还要预测出一个x坐标的相对偏移量，公式如下：

$$ \begin{equation*}
o=\left( x_{side} \ -c^{x}_{a}\right) /w_{a} ,\ o^{*} =\left( x^{*}_{side} -c^{x}_{a}\right) /w_{a}
\end{equation*} $$

&#160; &#160; &#160; &#160;其中，\\(x_{side}\\)是预测的距离当前anchor最近的边的距离,可以是左侧边，也可以是右侧边，\\(x^{*}_{side}\\)是ground truth中最近边到当前anchor的距离，\\(c^{x}_{a}\\)是anchor中心的x坐标，\\(w_{a}\\)是anchor 的宽度，这里恒等于16。

6.对检测到的宽度为16的小矩形进行连接构造一个长文本检测结果。构造过程非常简单，对于检测出的小矩形按照一定规则合并，直到无法合并之后输出最终检测结果，合并规则如下：

&#160; &#160; &#160; &#160;如果小矩形\\(b_{j}\\)是\\(b_{i}\\)最近的相邻小矩形，且它们之间的距离小于50个pixel，并且它们两个竖直方向的overlap比例大于0.7，则把这两个小矩形合并成一个。对检测到的小矩形一直进行这种合并，直到无法合并为止。有一点值得注意的是，当合并到文本行最边缘的小矩形时不再以小矩形的宽度作为最边上的小矩形的边界，而是使用5中描述的side-refinement预测出的边界相对偏移值。

Loss函数





**参考资料**


 1.[Joint Face Detection and Alignment using Multi-task Cascaded Convolutional Networks][1]

 


  [1]: https://kpzhang93.github.io/MTCNN_face_detection_alignment/paper/spl.pdf